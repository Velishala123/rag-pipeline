{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOOKIPrmht-C",
        "outputId": "e53a1863-58bc-40b4-e12d-5657b12698a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.12)\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: openai in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.57.4)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.9.0.post1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.25)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.2.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.7.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\manil\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\manil\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\manil\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install langchain PyPDF2 openai faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRKV5Sv9iXRT",
        "outputId": "a9ad4ce2-3dac-4bf5-b127-ef068aaba5cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Aa7FDXSskEn0"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (172873762.py, line 56)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 56\u001b[1;36m\u001b[0m\n\u001b[1;33m    pdf_path = \"C:\\Users\\manil\\Desktop\\sithafal\\ORDER DATA.pdf\"  # Replace with your PDF file path\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n",
        "\n",
        "# 1. PDF Data Ingestion\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"Splits text into smaller chunks for vectorization.\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunks.append(text[i : i + chunk_size])\n",
        "    # print(chunks)\n",
        "    return chunks\n",
        "\n",
        "# 2. Vector Database and Embedding\n",
        "def create_vector_store(chunks, model_name):\n",
        "    \"\"\"Converts text chunks into vector embeddings and stores them in FAISS.\"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks)\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings, chunks\n",
        "\n",
        "def query_vector_store(index, embeddings, chunks, query, model_name, top_k=3):\n",
        "    \"\"\"Queries the vector store to retrieve relevant chunks.\"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_vector = model.encode([query])\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "# 3. LLM for Response Generation\n",
        "def generate_response(retrieved_chunks, query):\n",
        "    \"\"\"Generates a response using an open-source LLM.\"\"\"\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Using Hugging Face Transformers pipeline for LLM response\n",
        "    qa_pipeline = pipeline(\"text-generation\", model=\"gpt2\")  # Replace with a larger model like GPT4All\n",
        "    # Use max_new_tokens instead of max_length to specify the desired length for the generated response\n",
        "    response = qa_pipeline(prompt, max_new_tokens=200, num_return_sequences=1)\n",
        "    return response[0][\"generated_text\"]\n",
        "# Example Workflow\n",
        "\n",
        "pdf_path = \"C:\\Users\\manil\\Desktop\\sithafal\\ORDER DATA.pdf\"  # Replace with your PDF file path\n",
        "model_name = \"all-MiniLM-L6-v2\"  # Open-source embedding model\n",
        "\n",
        "# Step 1: Extract text from PDF and split into chunks\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = split_text_into_chunks(text)\n",
        "\n",
        "# Step 2: Create vector store\n",
        "vector_store, embeddings, chunks = create_vector_store(chunks, model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eixs40XolbFZ",
        "outputId": "55d6d1d5-0a32-427b-dd29-ec97f4d03608"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query_vector_store' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: Handle user query\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwho is team lead of this project?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m retrieved_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mquery_vector_store\u001b[49m(vector_store, embeddings, chunks, query, model_name)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 4: Generate response\u001b[39;00m\n\u001b[0;32m      6\u001b[0m response \u001b[38;5;241m=\u001b[39m generate_response(retrieved_chunks, query)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'query_vector_store' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 3: Handle user query\n",
        "query = \"who is team lead of this project?\"\n",
        "retrieved_chunks = query_vector_store(vector_store, embeddings, chunks, query, model_name)\n",
        "\n",
        "# Step 4: Generate response\n",
        "response = generate_response(retrieved_chunks, query)\n",
        "print(\"Response:\", response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQGqpDZjrc5h",
        "outputId": "ee5b8953-489d-4bdb-e3a1-4c61203e737a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sentence_transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 1. PDF Data Ingestion\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
        "    \"\"\"Splits text into smaller chunks for vectorization.\"\"\"\n",
        "    chunks = []\n",
        "    for i in range(0, len(text), chunk_size - overlap):\n",
        "        chunks.append(text[i : i + chunk_size])\n",
        "    return chunks\n",
        "\n",
        "# 2. Vector Database and Embedding\n",
        "def create_vector_store(chunks, model_name):\n",
        "    \"\"\"Converts text chunks into vector embeddings and stores them in FAISS.\"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks)\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index, embeddings, chunks\n",
        "\n",
        "def query_vector_store(index, chunks, query, model_name, top_k=5):\n",
        "    \"\"\"Queries the vector store to retrieve relevant chunks.\"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_vector = model.encode([query])\n",
        "    distances, indices = index.search(query_vector, top_k)\n",
        "    return [chunks[i] for i in indices[0]]\n",
        "\n",
        "# 3. LLM for Response Generation\n",
        "def generate_response(retrieved_chunks, query, model_name=\"google/flan-t5-large\"):\n",
        "    \"\"\"Generates a response using an open-source LLM.\"\"\"\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"Answer the question based on the following context:\\n\\n{context}\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Load Hugging Face model for text generation\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids, max_length=300, num_return_sequences=1, temperature=0.3\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Example Workflow\n",
        "\n",
        "# Replace with your PDF file path\n",
        "pdf_path = \"/content/MINI PROJECT FINAL DOCUMENTATION __2024.pdf\"  # Ensure this is a valid PDF file in the working directory\n",
        "embedding_model = \"all-MiniLM-L6-v2\"  # Open-source embedding model\n",
        "\n",
        "# Step 1: Extract text from PDF and split into chunks\n",
        "print(\"Extracting text from PDF...\")\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "chunks = split_text_into_chunks(text)\n",
        "\n",
        "# Step 2: Create vector store\n",
        "print(\"Creating vector store...\")\n",
        "vector_store, embeddings, chunks = create_vector_store(chunks, embedding_model)\n",
        "print(vector_store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejJJcSjDr9-U",
        "outputId": "ac7a05db-7a69-43bf-d62a-4db6d22f9fd6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 3: Handle user query\n",
        "query = \"Machine Learning Libraries\"  # Example query\n",
        "print(\"Retrieving relevant chunks...\")\n",
        "retrieved_chunks = query_vector_store(vector_store, chunks, query, embedding_model)\n",
        "\n",
        "# Step 4: Generate response\n",
        "print(\"Generating response...\")\n",
        "response = generate_response(retrieved_chunks, query)\n",
        "print(\"Response:\", response)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAFNE5HFrhQk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
